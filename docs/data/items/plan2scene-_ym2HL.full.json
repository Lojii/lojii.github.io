{
  "id": "plan2scene-_ym2HL",
  "type": "repo",
  "name": "Plan2Scene",
  "nameEn": "Plan2Scene",
  "url": "https://github.com/3dlg-hcvc/plan2scene",
  "summary": "Plan2Scene：将平面图转换为 3D 场景",
  "description": "",
  "notes": "",
  "images": [
    "/assets/images/plan2scene-_ym2HL/1.jpg"
  ],
  "thumbnail": "/assets/images/plan2scene-_ym2HL/thumb.jpg",
  "category": "article",
  "tags": [
    "cad"
  ],
  "github": {
    "stars": 587,
    "forks": 77,
    "language": "Python",
    "license": "MIT",
    "lastUpdate": "2026-02-26",
    "topics": [
      "3d-reconstruction",
      "computer-vision",
      "indoor-reconstruction",
      "machine-learning",
      "texture-synthesis"
    ],
    "createdAt": "2021-03-18"
  },
  "archived": false,
  "createdAt": "2026-02-27T06:10:36.790Z",
  "updatedAt": "2026-02-27T06:10:36.790Z",
  "originalContent": "<div id=\"readme\" class=\"md\" data-path=\"README.md\"><article class=\"markdown-body entry-content container-lg\" itemprop=\"text\"><div class=\"markdown-heading\" dir=\"auto\"><h1 class=\"heading-element\" dir=\"auto\">Plan2Scene</h1><a id=\"user-content-plan2scene\" class=\"anchor\" aria-label=\"Permalink: Plan2Scene\" href=\"#plan2scene\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">Official repository of the paper:</p>\n<p dir=\"auto\"><strong>Plan2Scene: Converting floorplans to 3D scenes</strong></p>\n<p dir=\"auto\"><a href=\"https://github.com/madhawav\">Madhawa Vidanapathirana</a>, <a href=\"\">Qirui Wu</a>, <a href=\"\">Yasutaka Furukawa</a>, <a href=\"https://github.com/angelxuanchang\">Angel X. Chang</a>\n, <a href=\"https://github.com/msavva\">Manolis Savva</a></p>\n<p dir=\"auto\">[<a href=\"https://arxiv.org/abs/2106.05375\" rel=\"nofollow\">Paper</a>, <a href=\"https://3dlg-hcvc.github.io/plan2scene/\" rel=\"nofollow\">Project Page</a>, <a href=\"https://colab.research.google.com/drive/1lDkbfIV0drR1o9D0WYzoWeRskB91nXHq?usp=sharing\" rel=\"nofollow\">Google Colab Demo</a>]</p>\n<p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/3dlg-hcvc/plan2scene/blob/HEAD/docs/img/task-overview.png\"><img src=\"https://raw.githubusercontent.com/3dlg-hcvc/plan2scene/HEAD/docs/img/task-overview.png\" alt=\"Task Overview\" style=\"max-width: 100%;\"></a>\nIn the Plan2Scene task, we produce a textured 3D mesh of a residence from a floorplan and set of photos.</p>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Dependencies</h2><a id=\"user-content-dependencies\" class=\"anchor\" aria-label=\"Permalink: Dependencies\" href=\"#dependencies\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<ol dir=\"auto\">\n<li>We use a conda environment initialized as <a href=\"https://github.com/3dlg-hcvc/plan2scene/blob/HEAD/./docs/md/conda_env_setup.md\">described here</a>.</li>\n<li>Setup the <code>command line library</code> of <a href=\"https://github.com/EmbarkStudios/texture-synthesis#command-line-binary\">Embark Studios texture-synthesis</a> project.\n<ol dir=\"auto\">\n<li>You can download a pre-built binary <a href=\"https://github.com/EmbarkStudios/texture-synthesis/releases\">available here</a>. Alternatively, you may build from the source.</li>\n<li>Download the seam mask <a href=\"https://github.com/EmbarkStudios/texture-synthesis/blob/main/imgs/masks/1_tile.jpg\">available here</a>.</li>\n<li>Rename <code>./conf/plan2scene/seam_correct-example.json</code> to 'seam_correct.json' and update the paths to the texture synthesis command line library binary, and the seam mask.</li>\n</ol>\n</li>\n</ol>\n<p dir=\"auto\">Use 'code/src' as the source root when running python scripts.</p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"export PYTHONPATH=./code/src\"><pre><span class=\"pl-k\">export</span> PYTHONPATH=./code/src</pre></div>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Data</h2><a id=\"user-content-data\" class=\"anchor\" aria-label=\"Permalink: Data\" href=\"#data\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<ol dir=\"auto\">\n<li>\n<p dir=\"auto\">Rent3D++ dataset</p>\n<ol dir=\"auto\">\n<li>\n<p dir=\"auto\">Download and copy the <a href=\"https://forms.gle/mKAmnrzAm3LCK9ua6\" rel=\"nofollow\">Rent3D++ dataset</a> to the <code>[PROJECT_ROOT]/data</code> directory. The data organization is <a href=\"https://github.com/3dlg-hcvc/plan2scene/blob/HEAD/docs/md/rent3dpp_data_organization.md\">described here</a>.</p>\n</li>\n<li>\n<p dir=\"auto\">[Optional] We have provided 3D scenes pre-populated with CAD models of objects.\nIf you wish to re-populate these scenes using the <em>Object Placement</em> approach we use, <a href=\"https://github.com/3dlg-hcvc/plan2scene/blob/HEAD/docs/md/place_cad_models.md\">follow the instructions here</a>.</p>\n</li>\n<li>\n<p dir=\"auto\">To replicate our results, you should use the pre-extracted crops we provide.\nThese crops are provided with the Rent3D++ dataset and are copied to the <code>./data/processed/surface_crops</code> directory.</p>\n<p dir=\"auto\">[Optional] If you wish to extract new crops instead of using these provided crops, following <a href=\"https://github.com/3dlg-hcvc/plan2scene/blob/HEAD/./docs/md/extract_crops.md\">these instructions</a>.</p>\n</li>\n<li>\n<p dir=\"auto\">Select ground truth reference crops and populate photo room assignment lists.</p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"# Select ground truth reference crops.\npython code/scripts/plan2scene/preprocessing/generate_reference_crops.py ./data/processed/gt_reference/train ./data/input/photo_assignments/train train\npython code/scripts/plan2scene/preprocessing/generate_reference_crops.py ./data/processed/gt_reference/val ./data/input/photo_assignments/val val\npython code/scripts/plan2scene/preprocessing/generate_reference_crops.py ./data/processed/gt_reference/test ./data/input/photo_assignments/test test \n\n# We evaluate Plan2Scene by simulating photo un-observations.\n# Generate photoroom.csv files considering different photo un-observation ratios.\npython code/scripts/plan2scene/preprocessing/generate_unobserved_photo_assignments.py ./data/processed/photo_assignments/train ./data/input/photo_assignments/train ./data/input/unobserved_photos.json train\npython code/scripts/plan2scene/preprocessing/generate_unobserved_photo_assignments.py ./data/processed/photo_assignments/val ./data/input/photo_assignments/val ./data/input/unobserved_photos.json val\npython code/scripts/plan2scene/preprocessing/generate_unobserved_photo_assignments.py ./data/processed/photo_assignments/test ./data/input/photo_assignments/test ./data/input/unobserved_photos.json test  \"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Select ground truth reference crops.</span>\npython code/scripts/plan2scene/preprocessing/generate_reference_crops.py ./data/processed/gt_reference/train ./data/input/photo_assignments/train train\npython code/scripts/plan2scene/preprocessing/generate_reference_crops.py ./data/processed/gt_reference/val ./data/input/photo_assignments/val val\npython code/scripts/plan2scene/preprocessing/generate_reference_crops.py ./data/processed/gt_reference/test ./data/input/photo_assignments/test <span class=\"pl-c1\">test</span> \n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> We evaluate Plan2Scene by simulating photo un-observations.</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Generate photoroom.csv files considering different photo un-observation ratios.</span>\npython code/scripts/plan2scene/preprocessing/generate_unobserved_photo_assignments.py ./data/processed/photo_assignments/train ./data/input/photo_assignments/train ./data/input/unobserved_photos.json train\npython code/scripts/plan2scene/preprocessing/generate_unobserved_photo_assignments.py ./data/processed/photo_assignments/val ./data/input/photo_assignments/val ./data/input/unobserved_photos.json val\npython code/scripts/plan2scene/preprocessing/generate_unobserved_photo_assignments.py ./data/processed/photo_assignments/test ./data/input/photo_assignments/test ./data/input/unobserved_photos.json <span class=\"pl-c1\">test</span>  </pre></div>\n</li>\n</ol>\n</li>\n<li>\n<p dir=\"auto\">[Optional] Stationary Textures Dataset - We use one of the following datasets to train the texture synthesis model.\n<em>Not required if you are using pre-trained models.</em></p>\n<ul dir=\"auto\">\n<li><strong>Version 1</strong>: We use this dataset in our CVPR paper. Details are available <a href=\"https://github.com/3dlg-hcvc/plan2scene/blob/HEAD/./docs/md/stationary_textures_dataset_v1.md\">here</a>.</li>\n<li><strong>Version 2</strong>: Updated textures dataset which provides improved results on the Rent3D++ dataset. Details are available <a href=\"https://github.com/3dlg-hcvc/plan2scene/blob/HEAD/./docs/md/stationary_textures_dataset_v2.md\">here</a>.</li>\n</ul>\n</li>\n<li>\n<p dir=\"auto\">[Optional] <a href=\"https://github.com/3dlg-hcvc/plan2scene/blob/HEAD/./docs/md/smt_dataset.md\">Substance Mapped Textures dataset</a>. <em>Only used by the retrieve baseline.</em></p>\n</li>\n</ol>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Pretrained models</h2><a id=\"user-content-pretrained-models\" class=\"anchor\" aria-label=\"Permalink: Pretrained models\" href=\"#pretrained-models\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">Pretrained models are available <a href=\"https://github.com/3dlg-hcvc/plan2scene/blob/HEAD/./docs/md/pretrained_models.md\">here</a>.</p>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Inference on Rent3D++ dataset</h2><a id=\"user-content-inference-on-rent3d-dataset\" class=\"anchor\" aria-label=\"Permalink: Inference on Rent3D++ dataset\" href=\"#inference-on-rent3d-dataset\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<ol dir=\"auto\">\n<li>\n<p dir=\"auto\">Download and pre-process the Rent3D++ dataset as described in the data section.</p>\n</li>\n<li>\n<p dir=\"auto\">Setup a <a href=\"https://github.com/3dlg-hcvc/plan2scene/blob/HEAD/./docs/md/pretrained_models.md\">pretrained model</a> or train a new Plan2Scene network.</p>\n</li>\n<li>\n<p dir=\"auto\">Synthesize textures for observed surfaces using the VGG textureness score.</p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"# For test data without simulating photo unobservations. (drop = 0.0)\npython code/scripts/plan2scene/preprocessing/fill_room_embeddings.py ./data/processed/texture_gen/test/drop_0.0 test --drop 0.0\npython code/scripts/plan2scene/crop_select/vgg_crop_selector.py ./data/processed/vgg_crop_select/test/drop_0.0 ./data/processed/texture_gen/test/drop_0.0 test --drop 0.0\n# Results are stored at ./data/processed/vgg_crop_select/test/drop_0.0\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> For test data without simulating photo unobservations. (drop = 0.0)</span>\npython code/scripts/plan2scene/preprocessing/fill_room_embeddings.py ./data/processed/texture_gen/test/drop_0.0 <span class=\"pl-c1\">test</span> --drop 0.0\npython code/scripts/plan2scene/crop_select/vgg_crop_selector.py ./data/processed/vgg_crop_select/test/drop_0.0 ./data/processed/texture_gen/test/drop_0.0 <span class=\"pl-c1\">test</span> --drop 0.0\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Results are stored at ./data/processed/vgg_crop_select/test/drop_0.0</span></pre></div>\n</li>\n<li>\n<p dir=\"auto\">Propagate textures to unobserved surfaces using our texture propagation network.</p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"python code/scripts/plan2scene/texture_prop/gnn_texture_prop.py ./data/processed/gnn_prop/test/drop_0.0 ./data/processed/vgg_crop_select/test/drop_0.0 test GNN_PROP_CONF_PATH GNN_PROP_CHECKPOINT_PATH --keep-existing-predictions --drop 0.0\"><pre>python code/scripts/plan2scene/texture_prop/gnn_texture_prop.py ./data/processed/gnn_prop/test/drop_0.0 ./data/processed/vgg_crop_select/test/drop_0.0 <span class=\"pl-c1\">test</span> GNN_PROP_CONF_PATH GNN_PROP_CHECKPOINT_PATH --keep-existing-predictions --drop 0.0</pre></div>\n<p dir=\"auto\">To preview results, follow the instructions below.</p>\n</li>\n</ol>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Previewing outputs</h2><a id=\"user-content-previewing-outputs\" class=\"anchor\" aria-label=\"Permalink: Previewing outputs\" href=\"#previewing-outputs\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<ol dir=\"auto\">\n<li>Complete inference steps.</li>\n<li>Correct seams of predicted textures and make them tileable.\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"# For test data without simulating photo unobservations.\npython code/scripts/plan2scene/postprocessing/seam_correct_textures.py ./data/processed/gnn_prop/test/drop_0.0/tileable_texture_crops ./data/processed/gnn_prop/test/drop_0.0/texture_crops test --drop 0.0\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> For test data without simulating photo unobservations.</span>\npython code/scripts/plan2scene/postprocessing/seam_correct_textures.py ./data/processed/gnn_prop/test/drop_0.0/tileable_texture_crops ./data/processed/gnn_prop/test/drop_0.0/texture_crops <span class=\"pl-c1\">test</span> --drop 0.0</pre></div>\n</li>\n<li>Generate .scene.json files with embedded textures using <a href=\"https://github.com/3dlg-hcvc/plan2scene/blob/HEAD/code/scripts/plan2scene/postprocessing/embed_textures.py\">embed_textures.py</a>.\nA scene.json file describes the 3D geometry of a house.\nIt can be previewed via a browser using the 'scene-viewer' of <a href=\"https://github.com/smartscenes/sstk\">SmartScenesToolkit</a> (You will have to clone and build the SmartScenesToolkit).\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"# For test data without simulating photo unobservations.\npython code/scripts/plan2scene/postprocessing/embed_textures.py ./data/processed/gnn_prop/test/drop_0.0/archs ./data/processed/gnn_prop/test/drop_0.0/tileable_texture_crops test --drop 0.0\n# scene.json files are created in the ./data/processed/gnn_prop/test/drop_0.0/archs directory.\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> For test data without simulating photo unobservations.</span>\npython code/scripts/plan2scene/postprocessing/embed_textures.py ./data/processed/gnn_prop/test/drop_0.0/archs ./data/processed/gnn_prop/test/drop_0.0/tileable_texture_crops <span class=\"pl-c1\">test</span> --drop 0.0\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> scene.json files are created in the ./data/processed/gnn_prop/test/drop_0.0/archs directory.</span></pre></div>\n</li>\n<li>Render .scene.json files as .pngs using <a href=\"https://github.com/3dlg-hcvc/plan2scene/blob/HEAD/code/scripts/plan2scene/render_house_jsons.py\">render_house_jsons.py</a>.\n<ul dir=\"auto\">\n<li>Download and build the <a href=\"https://github.com/smartscenes/sstk\">SmartScenesToolkit</a>.</li>\n<li>Rename <code>./conf/render-example.json</code> to <code>./conf/render.json</code> and update its fields to point to scene-toolkit.</li>\n<li>Run the following command to generate previews.\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"CUDA_VISIBLE_DEVICES=0 python code/scripts/plan2scene/render_house_jsons.py ./data/processed/gnn_prop/test/drop_0.0/archs --scene-json\n# A .png file is created for each .scene.json file in the ./data/processed/gnn_prop/test/drop_0.0/archs directory.\"><pre>CUDA_VISIBLE_DEVICES=0 python code/scripts/plan2scene/render_house_jsons.py ./data/processed/gnn_prop/test/drop_0.0/archs --scene-json\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> A .png file is created for each .scene.json file in the ./data/processed/gnn_prop/test/drop_0.0/archs directory.</span></pre></div>\n</li>\n</ul>\n</li>\n<li>Generate qualitative result pages with previews using <a href=\"https://github.com/3dlg-hcvc/plan2scene/blob/HEAD/code/scripts/plan2scene/preview_houses.py\">preview_houses.py</a>.\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"python code/scripts/plan2scene/preview_houses.py ./data/processed/gnn_prop/test/drop_0.0/previews ./data/processed/gnn_prop/test/drop_0.0/archs ./data/input/photos test --textures-path ./data/processed/gnn_prop/test/drop_0.0/tileable_texture_crops 0.0\n# Open ./data/processed/gnn_prop/test/drop_0.0/previews/preview.html\"><pre>python code/scripts/plan2scene/preview_houses.py ./data/processed/gnn_prop/test/drop_0.0/previews ./data/processed/gnn_prop/test/drop_0.0/archs ./data/input/photos <span class=\"pl-c1\">test</span> --textures-path ./data/processed/gnn_prop/test/drop_0.0/tileable_texture_crops 0.0\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Open ./data/processed/gnn_prop/test/drop_0.0/previews/preview.html</span></pre></div>\n</li>\n</ol>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Test on Rent3D++ dataset</h2><a id=\"user-content-test-on-rent3d-dataset\" class=\"anchor\" aria-label=\"Permalink: Test on Rent3D++ dataset\" href=\"#test-on-rent3d-dataset\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<ol dir=\"auto\">\n<li>\n<p dir=\"auto\">[Optional] Download a pre-trained model or train the substance classifier used by the Subs metric.\nTraining instructions are available <a href=\"https://github.com/3dlg-hcvc/plan2scene/blob/HEAD/./docs/md/train_substance_classifier.md\">here</a>.\nPre-trained weights are available <a href=\"https://github.com/3dlg-hcvc/plan2scene/blob/HEAD/./docs/md/pretrained_models.md\">here</a>.\nSkip this step to omit the Subs metric.</p>\n</li>\n<li>\n<p dir=\"auto\">Generate overall evaluation report at 60% photo unobservations. We used this setting in paper evaluations.</p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"# Synthesize textures for observed surfaces using the VGG textureness score.\n# For the case: 60% (i.e. 0.6) of the photos unobserved. \npython code/scripts/plan2scene/preprocessing/fill_room_embeddings.py ./data/processed/texture_gen/test/drop_0.6 test --drop 0.6\npython code/scripts/plan2scene/crop_select/vgg_crop_selector.py ./data/processed/vgg_crop_select/test/drop_0.6 ./data/processed/texture_gen/test/drop_0.6 test --drop 0.6\n\n# Propagate textures to un-observed surfaces using our GNN.\n# For the case: 60% (i.e. 0.6) of the photos unobserved.\npython code/scripts/plan2scene/texture_prop/gnn_texture_prop.py ./data/processed/gnn_prop/test/drop_0.6 ./data/processed/vgg_crop_select/test/drop_0.6 test GNN_PROP_CONF_PATH GNN_PROP_CHECKPOINT_PATH --keep-existing-predictions --drop 0.6\n\n# Correct seams of texture crops and make them tileable.\n# For test data where 60% of photos are unobserved.\npython code/scripts/plan2scene/postprocessing/seam_correct_textures.py ./data/processed/gnn_prop/test/drop_0.6/tileable_texture_crops ./data/processed/gnn_prop/test/drop_0.6/texture_crops test --drop 0.6\n\n# Generate overall results at 60% simulated photo unobservations.\npython code/scripts/plan2scene/test.py ./data/processed/gnn_prop/test/drop_0.6/tileable_texture_crops ./data/processed/gt_reference/test/texture_crops test\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Synthesize textures for observed surfaces using the VGG textureness score.</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> For the case: 60% (i.e. 0.6) of the photos unobserved. </span>\npython code/scripts/plan2scene/preprocessing/fill_room_embeddings.py ./data/processed/texture_gen/test/drop_0.6 <span class=\"pl-c1\">test</span> --drop 0.6\npython code/scripts/plan2scene/crop_select/vgg_crop_selector.py ./data/processed/vgg_crop_select/test/drop_0.6 ./data/processed/texture_gen/test/drop_0.6 <span class=\"pl-c1\">test</span> --drop 0.6\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Propagate textures to un-observed surfaces using our GNN.</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> For the case: 60% (i.e. 0.6) of the photos unobserved.</span>\npython code/scripts/plan2scene/texture_prop/gnn_texture_prop.py ./data/processed/gnn_prop/test/drop_0.6 ./data/processed/vgg_crop_select/test/drop_0.6 <span class=\"pl-c1\">test</span> GNN_PROP_CONF_PATH GNN_PROP_CHECKPOINT_PATH --keep-existing-predictions --drop 0.6\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Correct seams of texture crops and make them tileable.</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> For test data where 60% of photos are unobserved.</span>\npython code/scripts/plan2scene/postprocessing/seam_correct_textures.py ./data/processed/gnn_prop/test/drop_0.6/tileable_texture_crops ./data/processed/gnn_prop/test/drop_0.6/texture_crops <span class=\"pl-c1\">test</span> --drop 0.6\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Generate overall results at 60% simulated photo unobservations.</span>\npython code/scripts/plan2scene/test.py ./data/processed/gnn_prop/test/drop_0.6/tileable_texture_crops ./data/processed/gt_reference/test/texture_crops <span class=\"pl-c1\">test</span></pre></div>\n</li>\n<li>\n<p dir=\"auto\">Generate evaluation report for observed surfaces. No simulated unobservation of photos. We used this setting in paper evaluations.</p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"# Run inference on using drop=0.0.\npython code/scripts/plan2scene/preprocessing/fill_room_embeddings.py ./data/processed/texture_gen/test/drop_0.0 test --drop 0.0\npython code/scripts/plan2scene/crop_select/vgg_crop_selector.py ./data/processed/vgg_crop_select/test/drop_0.0 ./data/processed/texture_gen/test/drop_0.0 test --drop 0.0\n\n# Correct seams of texture crops and make them tileable by running seam_correct_textures.py.\npython code/scripts/plan2scene/postprocessing/seam_correct_textures.py ./data/processed/vgg_crop_select/test/drop_0.0/tileable_texture_crops ./data/processed/vgg_crop_select/test/drop_0.0/texture_crops test --drop 0.0\n\n# Generate evaluation results for observed surfaces.\npython code/scripts/plan2scene/test.py ./data/processed/vgg_crop_select/test/drop_0.0/tileable_texture_crops ./data/processed/gt_reference/test/texture_crops test\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Run inference on using drop=0.0.</span>\npython code/scripts/plan2scene/preprocessing/fill_room_embeddings.py ./data/processed/texture_gen/test/drop_0.0 <span class=\"pl-c1\">test</span> --drop 0.0\npython code/scripts/plan2scene/crop_select/vgg_crop_selector.py ./data/processed/vgg_crop_select/test/drop_0.0 ./data/processed/texture_gen/test/drop_0.0 <span class=\"pl-c1\">test</span> --drop 0.0\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Correct seams of texture crops and make them tileable by running seam_correct_textures.py.</span>\npython code/scripts/plan2scene/postprocessing/seam_correct_textures.py ./data/processed/vgg_crop_select/test/drop_0.0/tileable_texture_crops ./data/processed/vgg_crop_select/test/drop_0.0/texture_crops <span class=\"pl-c1\">test</span> --drop 0.0\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Generate evaluation results for observed surfaces.</span>\npython code/scripts/plan2scene/test.py ./data/processed/vgg_crop_select/test/drop_0.0/tileable_texture_crops ./data/processed/gt_reference/test/texture_crops <span class=\"pl-c1\">test</span></pre></div>\n</li>\n<li>\n<p dir=\"auto\">Generate evaluation report for unobserved surfaces at 60% photo unobservations. We used this setting in the paper evaluations.</p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"# It is assumed that the user has already generated the overall report at 0.6 drop fraction.\n\n# Generate results on unobserved surfaces at 60% simulated photo unobservations.\npython code/scripts/plan2scene/test.py ./data/processed/gnn_prop/test/drop_0.6/tileable_texture_crops ./data/processed/gt_reference/test/texture_crops test --exclude-prior-predictions ./data/processed/vgg_crop_select/test/drop_0.6/texture_crops\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> It is assumed that the user has already generated the overall report at 0.6 drop fraction.</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Generate results on unobserved surfaces at 60% simulated photo unobservations.</span>\npython code/scripts/plan2scene/test.py ./data/processed/gnn_prop/test/drop_0.6/tileable_texture_crops ./data/processed/gt_reference/test/texture_crops <span class=\"pl-c1\">test</span> --exclude-prior-predictions ./data/processed/vgg_crop_select/test/drop_0.6/texture_crops</pre></div>\n</li>\n<li>\n<p dir=\"auto\">Generate evaluation report on FID metric as described <a href=\"https://github.com/3dlg-hcvc/plan2scene/blob/HEAD/./docs/md/compute_fid_metric.md\">here</a>.</p>\n</li>\n</ol>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Inference on custom data</h2><a id=\"user-content-inference-on-custom-data\" class=\"anchor\" aria-label=\"Permalink: Inference on custom data\" href=\"#inference-on-custom-data\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">If you have scanned images of floorplans, you can use <a href=\"https://github.com/art-programmer/FloorplanTransformation\">raster-to-vector</a> to convert those floorplan images to a vector format. Then, follow the <a href=\"https://github.com/3dlg-hcvc/plan2scene/blob/HEAD/./docs/md/plan2scene_on_r2v.md\">instructions here</a> to create textured 3D meshes of houses.</p>\n<p dir=\"auto\">If you have floorplan vectors in another format, you can convert them to the raster-to-vector <strong>annotation format</strong>.\nThen, follow the same instructions as before to create textured 3D meshes of houses.\nThe R2V annotation format is explained with examples in the <a href=\"https://github.com/art-programmer/FloorplanTransformation#data\">data section of the raster-to-vector repository</a>.</p>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Training a new Plan2Scene network</h2><a id=\"user-content-training-a-new-plan2scene-network\" class=\"anchor\" aria-label=\"Permalink: Training a new Plan2Scene network\" href=\"#training-a-new-plan2scene-network\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">Plan2Scene consists of two trainable components, 1) the texture synthesis stage and 2) the texture propagation stage. Each stage is trained separately. The training procedure is as follows.</p>\n<ol dir=\"auto\">\n<li>Train the texture synthesis stage as described <a href=\"https://github.com/3dlg-hcvc/plan2scene/blob/HEAD/./docs/md/train_texture_synth.md\">here</a>.</li>\n<li>Train the texture propagation stage as described <a href=\"https://github.com/3dlg-hcvc/plan2scene/blob/HEAD/./docs/md/train_texture_prop.md\">here</a>.</li>\n</ol>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Baseline Models</h2><a id=\"user-content-baseline-models\" class=\"anchor\" aria-label=\"Permalink: Baseline Models\" href=\"#baseline-models\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">The baseline models are <a href=\"https://github.com/3dlg-hcvc/plan2scene/blob/HEAD/./docs/md/baselines.md\">available here</a>.</p>\n</article></div>"
}