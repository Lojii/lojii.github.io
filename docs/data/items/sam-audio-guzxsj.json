{
  "id": "sam-audio-guzxsj",
  "type": "repo",
  "name": "SAM-Audio ",
  "nameEn": "sam-audio",
  "url": "https://github.com/facebookresearch/sam-audio",
  "summary": "利用文本、视觉或时间提示来分离音频中的任何声音。它可以根据自然语言描述、视频中的视觉线索或时间跨度，从复杂的音频混合中分离出特定的声音。",
  "description": "",
  "notes": "",
  "images": [
    "/assets/images/sam-audio-guzxsj/1.jpg"
  ],
  "thumbnail": "/assets/images/sam-audio-guzxsj/thumb.jpg",
  "category": "ai",
  "tags": [
    "音频"
  ],
  "github": {
    "stars": 3217,
    "forks": 267,
    "language": "Python",
    "license": "NOASSERTION",
    "lastUpdate": "2026-01-28",
    "topics": [],
    "createdAt": "2025-09-04"
  },
  "archived": false,
  "createdAt": "2026-01-28T05:34:15.720Z",
  "updatedAt": "2026-01-28T05:34:15.720Z",
  "originalContent": "<div id=\"readme\" class=\"md\" data-path=\"README.md\"><article class=\"markdown-body entry-content container-lg\" itemprop=\"text\"><div align=\"center\" dir=\"auto\">\n<div class=\"markdown-heading\" dir=\"auto\"><h1 class=\"heading-element\" dir=\"auto\">SAM-Audio</h1><a id=\"user-content-sam-audio\" class=\"anchor\" aria-label=\"Permalink: SAM-Audio\" href=\"#sam-audio\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\"><a href=\"https://arxiv.org/abs/2512.18099\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/d0c637f9f2b14a2f261ce36632466f6b8db48a91826064434929cdf19b13a64c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323531322e31383039392d6233316231622e737667\" alt=\"arXiv\" data-canonical-src=\"https://img.shields.io/badge/arXiv-2512.18099-b31b1b.svg\" style=\"max-width: 100%;\"></a>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/facebookresearch/sam-audio/actions/workflows/ci.yaml/badge.svg\"><img src=\"https://github.com/facebookresearch/sam-audio/actions/workflows/ci.yaml/badge.svg\" alt=\"CI\" style=\"max-width: 100%;\"></a>\n<a href=\"https://huggingface.co/collections/facebook/sam-audio\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0ec1edc31da1db1027074ab138e375d6dd9b709ac3abedf38ae78c7a7f38e71c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f48756767696e67466163652d436f6c6c656374696f6e2d6f72616e67653f6c6f676f3d68756767696e6766616365\" alt=\"Hugging Face\" data-canonical-src=\"https://img.shields.io/badge/HuggingFace-Collection-orange?logo=huggingface\" style=\"max-width: 100%;\"></a></p>\n<p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/facebookresearch/sam-audio/blob/HEAD/assets/sam_audio_main_model.png\"><img src=\"https://raw.githubusercontent.com/facebookresearch/sam-audio/HEAD/assets/sam_audio_main_model.png\" alt=\"model_image\" style=\"max-width: 100%;\"></a></p>\n</div>\n<p dir=\"auto\">Segment Anything Model for Audio [<a href=\"https://ai.meta.com/blog/sam-audio/\" rel=\"nofollow\"><strong>Blog</strong></a>] [<a href=\"https://ai.meta.com/research/publications/sam-audio-segment-anything-in-audio/\" rel=\"nofollow\"><strong>Paper</strong></a>] [<a href=\"https://aidemos.meta.com/segment-anything/editor/segment-audio\" rel=\"nofollow\"><strong>Demo</strong></a>]</p>\n<p dir=\"auto\">SAM-Audio is a foundation model for isolating any sound in audio using text, visual, or temporal prompts. It can separate specific sounds from complex audio mixtures based on natural language descriptions, visual cues from video, or time spans.</p>\n<p dir=\"auto\">SAM-Audio and the Judge model crucially rely on <a href=\"https://huggingface.co/facebook/pe-av-large\" rel=\"nofollow\">Perception-Encoder Audio-Visual (PE-AV)</a>, which you can read more about <a href=\"https://ai.meta.com/research/publications/pushing-the-frontier-of-audiovisual-perception-with-large-scale-multimodal-correspondence-learning/\" rel=\"nofollow\">here</a></p>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Setup</h2><a id=\"user-content-setup\" class=\"anchor\" aria-label=\"Permalink: Setup\" href=\"#setup\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\"><strong>Requirements:</strong></p>\n<ul dir=\"auto\">\n<li>Python &gt;= 3.11</li>\n<li>CUDA-compatible GPU (recommended)</li>\n</ul>\n<p dir=\"auto\">Install dependencies:</p>\n<div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"pip install .\"><pre>pip install <span class=\"pl-c1\">.</span></pre></div>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Usage</h2><a id=\"user-content-usage\" class=\"anchor\" aria-label=\"Permalink: Usage\" href=\"#usage\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\"><g-emoji class=\"g-emoji\" alias=\"warning\">⚠️</g-emoji> Before using SAM Audio, please request access to the checkpoints on the SAM Audio\nHugging Face <a href=\"https://huggingface.co/facebook/sam-audio-large\" rel=\"nofollow\">repo</a>. Once accepted, you\nneed to be authenticated to download the checkpoints. You can do this by running\nthe following <a href=\"https://huggingface.co/docs/huggingface_hub/en/quick-start#authentication\" rel=\"nofollow\">steps</a>\n(e.g. <code>hf auth login</code> after generating an access token.)</p>\n<div class=\"markdown-heading\" dir=\"auto\"><h3 class=\"heading-element\" dir=\"auto\">Basic Text Prompting</h3><a id=\"user-content-basic-text-prompting\" class=\"anchor\" aria-label=\"Permalink: Basic Text Prompting\" href=\"#basic-text-prompting\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"from sam_audio import SAMAudio, SAMAudioProcessor\nimport torchaudio\nimport torch\n\nmodel = SAMAudio.from_pretrained(&quot;facebook/sam-audio-large&quot;)\nprocessor = SAMAudioProcessor.from_pretrained(&quot;facebook/sam-audio-large&quot;)\nmodel = model.eval().cuda()\n\nfile = &quot;&lt;audio file&gt;&quot; # audio file path or torch tensor\ndescription = &quot;&lt;description&gt;&quot;\n\nbatch = processor(\n    audios=[file],\n    descriptions=[description],\n).to(&quot;cuda&quot;)\n\nwith torch.inference_mode():\n    # NOTE: `predict_spans` and `reranking_candidates` have a large impact on performance.\n    # Setting `predict_span=True` and `reranking_candidates=8` will give you better results at the cost of\n    # latency and memory. See the &quot;Span Prediction&quot; section below for more details\n   result = model.separate(batch, predict_spans=False, reranking_candidates=1)\n\n# Save separated audio\nsample_rate = processor.audio_sampling_rate\ntorchaudio.save(&quot;target.wav&quot;, result.target.cpu(), sample_rate)      # The isolated sound\ntorchaudio.save(&quot;residual.wav&quot;, result.residual.cpu(), sample_rate)  # Everything else\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">sam_audio</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">SAMAudio</span>, <span class=\"pl-v\">SAMAudioProcessor</span>\n<span class=\"pl-k\">import</span> <span class=\"pl-s1\">torchaudio</span>\n<span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span>\n\n<span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">SAMAudio</span>.<span class=\"pl-c1\">from_pretrained</span>(<span class=\"pl-s\">\"facebook/sam-audio-large\"</span>)\n<span class=\"pl-s1\">processor</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">SAMAudioProcessor</span>.<span class=\"pl-c1\">from_pretrained</span>(<span class=\"pl-s\">\"facebook/sam-audio-large\"</span>)\n<span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">model</span>.<span class=\"pl-c1\">eval</span>().<span class=\"pl-c1\">cuda</span>()\n\n<span class=\"pl-s1\">file</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">\"&lt;audio file&gt;\"</span> <span class=\"pl-c\"># audio file path or torch tensor</span>\n<span class=\"pl-s1\">description</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">\"&lt;description&gt;\"</span>\n\n<span class=\"pl-s1\">batch</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">processor</span>(\n    <span class=\"pl-s1\">audios</span><span class=\"pl-c1\">=</span>[<span class=\"pl-s1\">file</span>],\n    <span class=\"pl-s1\">descriptions</span><span class=\"pl-c1\">=</span>[<span class=\"pl-s1\">description</span>],\n).<span class=\"pl-c1\">to</span>(<span class=\"pl-s\">\"cuda\"</span>)\n\n<span class=\"pl-k\">with</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-c1\">inference_mode</span>():\n    <span class=\"pl-c\"># NOTE: `predict_spans` and `reranking_candidates` have a large impact on performance.</span>\n    <span class=\"pl-c\"># Setting `predict_span=True` and `reranking_candidates=8` will give you better results at the cost of</span>\n    <span class=\"pl-c\"># latency and memory. See the \"Span Prediction\" section below for more details</span>\n   <span class=\"pl-s1\">result</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">model</span>.<span class=\"pl-c1\">separate</span>(<span class=\"pl-s1\">batch</span>, <span class=\"pl-s1\">predict_spans</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-s1\">reranking_candidates</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-c\"># Save separated audio</span>\n<span class=\"pl-s1\">sample_rate</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">processor</span>.<span class=\"pl-c1\">audio_sampling_rate</span>\n<span class=\"pl-s1\">torchaudio</span>.<span class=\"pl-c1\">save</span>(<span class=\"pl-s\">\"target.wav\"</span>, <span class=\"pl-s1\">result</span>.<span class=\"pl-c1\">target</span>.<span class=\"pl-c1\">cpu</span>(), <span class=\"pl-s1\">sample_rate</span>)      <span class=\"pl-c\"># The isolated sound</span>\n<span class=\"pl-s1\">torchaudio</span>.<span class=\"pl-c1\">save</span>(<span class=\"pl-s\">\"residual.wav\"</span>, <span class=\"pl-s1\">result</span>.<span class=\"pl-c1\">residual</span>.<span class=\"pl-c1\">cpu</span>(), <span class=\"pl-s1\">sample_rate</span>)  <span class=\"pl-c\"># Everything else</span></pre></div>\n<div class=\"markdown-heading\" dir=\"auto\"><h3 class=\"heading-element\" dir=\"auto\">Prompting Methods</h3><a id=\"user-content-prompting-methods\" class=\"anchor\" aria-label=\"Permalink: Prompting Methods\" href=\"#prompting-methods\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">SAM-Audio supports three types of prompts:</p>\n<ol dir=\"auto\">\n<li>\n<p dir=\"auto\"><strong>Text Prompting</strong>: Describe the sound you want to isolate using natural language. To match training, please use lowercase noun-phrase/verb-phrase (NP/VP) format for text (for example instead of \"Thunder can be heard in the background\" use \"thunder\").</p>\n<div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"processor(audios=[audio], descriptions=[&quot;man speaking&quot;])\"><pre><span class=\"pl-en\">processor</span>(<span class=\"pl-s1\">audios</span><span class=\"pl-c1\">=</span>[<span class=\"pl-s1\">audio</span>], <span class=\"pl-s1\">descriptions</span><span class=\"pl-c1\">=</span>[<span class=\"pl-s\">\"man speaking\"</span>])</pre></div>\n</li>\n<li>\n<p dir=\"auto\"><strong>Visual Prompting</strong>: Use video frames and masks to isolate sounds associated with visual objects</p>\n<div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"processor(audios=[video], descriptions=[&quot;&quot;], masked_videos=processor.mask_videos([frames], [mask]))\"><pre><span class=\"pl-en\">processor</span>(<span class=\"pl-s1\">audios</span><span class=\"pl-c1\">=</span>[<span class=\"pl-s1\">video</span>], <span class=\"pl-s1\">descriptions</span><span class=\"pl-c1\">=</span>[<span class=\"pl-s\">\"\"</span>], <span class=\"pl-s1\">masked_videos</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">processor</span>.<span class=\"pl-c1\">mask_videos</span>([<span class=\"pl-s1\">frames</span>], [<span class=\"pl-s1\">mask</span>]))</pre></div>\n</li>\n<li>\n<p dir=\"auto\"><strong>Span Prompting</strong>: Specify time ranges where the target sound occurs</p>\n<div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"processor(audios=[audio], descriptions=[&quot;car honking&quot;], anchors=[[[&quot;+&quot;, 6.3, 7.0]]])\"><pre><span class=\"pl-en\">processor</span>(<span class=\"pl-s1\">audios</span><span class=\"pl-c1\">=</span>[<span class=\"pl-s1\">audio</span>], <span class=\"pl-s1\">descriptions</span><span class=\"pl-c1\">=</span>[<span class=\"pl-s\">\"car honking\"</span>], <span class=\"pl-s1\">anchors</span><span class=\"pl-c1\">=</span>[[[<span class=\"pl-s\">\"+\"</span>, <span class=\"pl-c1\">6.3</span>, <span class=\"pl-c1\">7.0</span>]]])</pre></div>\n</li>\n</ol>\n<p dir=\"auto\">See the <a href=\"https://github.com/facebookresearch/sam-audio/blob/HEAD/examples\">examples</a> directory for more detailed examples</p>\n<div class=\"markdown-heading\" dir=\"auto\"><h3 class=\"heading-element\" dir=\"auto\">Span Prediction (Optional for Text Prompting)</h3><a id=\"user-content-span-prediction-optional-for-text-prompting\" class=\"anchor\" aria-label=\"Permalink: Span Prediction (Optional for Text Prompting)\" href=\"#span-prediction-optional-for-text-prompting\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">We also provide support for automatically predicting the spans based on the text description, which is especially helpful for separating non-ambience sound events.  You can enable this by adding <code>predict_spans=True</code> in your call to <code>separate</code></p>\n<div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"with torch.inference_mode()\n   outputs = model.separate(batch, predict_spans=True)\n\n# To further improve performance (at the expense of latency), you can add candidate re-ranking\nwith torch.inference_mode():\n   outputs = model.separate(batch, predict_spans=True, reranking_candidates=8)\"><pre><span class=\"pl-k\">with</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-c1\">inference_mode</span>()\n   <span class=\"pl-s1\">outputs</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">model</span>.<span class=\"pl-c1\">separate</span>(<span class=\"pl-s1\">batch</span>, <span class=\"pl-s1\">predict_spans</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-c\"># To further improve performance (at the expense of latency), you can add candidate re-ranking</span>\n<span class=\"pl-s1\">with</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-c1\">inference_mode</span>():\n   <span class=\"pl-s1\">outputs</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">model</span>.<span class=\"pl-c1\">separate</span>(<span class=\"pl-s1\">batch</span>, <span class=\"pl-s1\">predict_spans</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-s1\">reranking_candidates</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">8</span>)</pre></div>\n<div class=\"markdown-heading\" dir=\"auto\"><h3 class=\"heading-element\" dir=\"auto\">Re-Ranking</h3><a id=\"user-content-re-ranking\" class=\"anchor\" aria-label=\"Permalink: Re-Ranking\" href=\"#re-ranking\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">We provide the following models to assess the quality of the separated audio:</p>\n<ul dir=\"auto\">\n<li><a href=\"https://github.com/LAION-AI/CLAP\">CLAP</a>: measures the similarity between the target audio and text description</li>\n<li><a href=\"https://huggingface.co/facebook/sam-audio-judge\" rel=\"nofollow\">Judge</a>: measures the overall separation quality across 3 axes: precision, recall, and faithfulness (see the <a href=\"https://huggingface.co/facebook/sam-audio-judge#output-format\" rel=\"nofollow\">model card</a> for more details)</li>\n<li><a href=\"https://github.com/facebookresearch/ImageBind\">ImageBind</a>: for visual prompting, we measure the imagebind embedding similarity between the separated audio and the masked input video</li>\n</ul>\n<p dir=\"auto\">We provide support for generating multiple candidates (by setting <code>reranking_candidates=&lt;k&gt;</code> in your call to <code>separate</code>), which will generate <code>k</code> audios, and choose the best one based on the ranking models mentioned above</p>\n<div class=\"markdown-heading\" dir=\"auto\"><h1 class=\"heading-element\" dir=\"auto\">Models</h1><a id=\"user-content-models\" class=\"anchor\" aria-label=\"Permalink: Models\" href=\"#models\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">Below is a table of each of the models we released along with their overall subjective evaluation scores</p>\n<markdown-accessiblity-table><table>\n<thead>\n<tr>\n<th>Model</th>\n<th>General SFX</th>\n<th>Speech</th>\n<th>Speaker</th>\n<th>Music</th>\n<th>Instr(wild)</th>\n<th>Instr(pro)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://huggingface.co/facebook/sam-audio-small\" rel=\"nofollow\"><code>sam-audio-small</code></a></td>\n<td>3.62</td>\n<td>3.99</td>\n<td>3.12</td>\n<td>4.11</td>\n<td>3.56</td>\n<td>4.24</td>\n</tr>\n<tr>\n<td><a href=\"https://huggingface.co/facebook/sam-audio-base\" rel=\"nofollow\"><code>sam-audio-base</code></a></td>\n<td>3.28</td>\n<td>4.25</td>\n<td>3.57</td>\n<td>3.87</td>\n<td>3.66</td>\n<td>4.27</td>\n</tr>\n<tr>\n<td><a href=\"https://huggingface.co/facebook/sam-audio-large\" rel=\"nofollow\"><code>sam-audio-large</code></a></td>\n<td>3.50</td>\n<td>4.03</td>\n<td>3.60</td>\n<td>4.22</td>\n<td>3.66</td>\n<td>4.49</td>\n</tr>\n</tbody>\n</table></markdown-accessiblity-table>\n<p dir=\"auto\">We additional release another variant (in each size) that is better specifically on correctness of target sound as well as visual prompting:</p>\n<ul dir=\"auto\">\n<li><a href=\"https://huggingface.co/facebook/sam-audio-small-tv\" rel=\"nofollow\"><code>sam-audio-small-tv</code></a></li>\n<li><a href=\"https://huggingface.co/facebook/sam-audio-base-tv\" rel=\"nofollow\"><code>sam-audio-base-tv</code></a></li>\n<li><a href=\"https://huggingface.co/facebook/sam-audio-large-tv\" rel=\"nofollow\"><code>sam-audio-large-tv</code></a></li>\n</ul>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Evaluation</h2><a id=\"user-content-evaluation\" class=\"anchor\" aria-label=\"Permalink: Evaluation\" href=\"#evaluation\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">See the <a href=\"https://github.com/facebookresearch/sam-audio/blob/HEAD/eval\">eval</a> directory for instructions and scripts to reproduce results from the paper</p>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Contributing</h2><a id=\"user-content-contributing\" class=\"anchor\" aria-label=\"Permalink: Contributing\" href=\"#contributing\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">See <a href=\"https://github.com/facebookresearch/sam-audio/blob/HEAD/CONTRIBUTING.md\">contributing</a> and <a href=\"https://github.com/facebookresearch/sam-audio/blob/HEAD/CODE_OF_CONDUCT.md\">code of conduct</a> for more information.</p>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">License</h2><a id=\"user-content-license\" class=\"anchor\" aria-label=\"Permalink: License\" href=\"#license\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">This project is licensed under the SAM License - see the <a href=\"https://github.com/facebookresearch/sam-audio/blob/HEAD/LICENSE\">LICENSE</a> file for details.</p>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Citing SAM Audio</h2><a id=\"user-content-citing-sam-audio\" class=\"anchor\" aria-label=\"Permalink: Citing SAM Audio\" href=\"#citing-sam-audio\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">If you use SAM Audio in your research, please use the following BibTex entry:</p>\n<div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{shi2025samaudio,\n    title={SAM Audio: Segment Anything in Audio},\n    author={Bowen Shi and Andros Tjandra and John Hoffman and Helin Wang and Yi-Chiao Wu and Luya Gao and Julius Richter and Matt Le and Apoorv Vyas and Sanyuan Chen and Christoph Feichtenhofer and Piotr Doll{\\'a}r and Wei-Ning Hsu and Ann Lee},\n    year={2025},\n    url={https://arxiv.org/abs/2512.18099}\n}\"><pre><span class=\"pl-k\">@article</span>{<span class=\"pl-en\">shi2025samaudio</span>,\n    <span class=\"pl-s\">title</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>SAM Audio: Segment Anything in Audio<span class=\"pl-pds\">}</span></span>,\n    <span class=\"pl-s\">author</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>Bowen Shi and Andros Tjandra and John Hoffman and Helin Wang and Yi-Chiao Wu and Luya Gao and Julius Richter and Matt Le and Apoorv Vyas and Sanyuan Chen and Christoph Feichtenhofer and Piotr Doll{\\'a}r and Wei-Ning Hsu and Ann Lee<span class=\"pl-pds\">}</span></span>,\n    <span class=\"pl-s\">year</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>2025<span class=\"pl-pds\">}</span></span>,\n    <span class=\"pl-s\">url</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://arxiv.org/abs/2512.18099<span class=\"pl-pds\">}</span></span>\n}</pre></div>\n</article></div>"
}